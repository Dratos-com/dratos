{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2ad72c977d34f46a94d3477cfb4763e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import json\n",
    "from beta import Artifact\n",
    "\n",
    "def on_file_selected(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        file_content = change['new']\n",
    "        if not file_content:\n",
    "            print(\"No file selected\")\n",
    "            return\n",
    "        \n",
    "        print(\"File content type:\", type(file_content))\n",
    "        print(\"File content:\", file_content)\n",
    "        \n",
    "        # Handle the tuple structure\n",
    "        if isinstance(file_content, tuple) and len(file_content) > 0:\n",
    "            file_data = file_content[0]\n",
    "            file_name = file_data['name']\n",
    "            \n",
    "            print(f\"Selected file: {file_name}\")\n",
    "            print(\"File data type:\", type(file_data))\n",
    "            \n",
    "            try:\n",
    "                # Create the Artifact\n",
    "                artifact = Artifact(files=[file_name])\n",
    "                print(\"Artifact created successfully\")\n",
    "                print(\"Artifact DataFrame:\")\n",
    "                artifact.df.collect().show()\n",
    "            except Exception as e:\n",
    "                print(f\"Error creating Artifact: {str(e)}\")\n",
    "        else:\n",
    "            print(\"Unexpected file content format\")\n",
    "\n",
    "# Create a file upload widget\n",
    "file_upload = widgets.FileUpload(\n",
    "    accept='',  # Accept all file types\n",
    "    multiple=False  # Allow only single file selection\n",
    ")\n",
    "\n",
    "# Display the file upload widget\n",
    "display(file_upload)\n",
    "\n",
    "# Attach the callback to the file upload widget\n",
    "file_upload.observe(on_file_selected, names='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from beta.agents import Agent\n",
    "from beta.models.obj.model_object import VLLM\n",
    "from vllm import LLM\n",
    "from beta.tools.obj import CalculatorTool, DataFrameTool\n",
    "from beta.models.serve.engines import VLLMEngine\n",
    "from beta.models.serve.engines import OpenAIEngineConfig\n",
    "from mlflow.client import MlflowClient\n",
    "from daft import DataFrame\n",
    "from beta.models.serve.engines.openai_engine import OpenAIEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:daft_io.stats:IOStatsContext: MicroPartition::concat, Gets: 0, Heads: 0, Lists: 0, BytesRead: 0, AvgGetSize: 0, BytesUploaded: 0, AvgPutSize: 0\n",
      "INFO:daft_io.stats:IOStatsContext: MicroPartition::concat, Gets: 0, Heads: 0, Lists: 0, BytesRead: 0, AvgGetSize: 0, BytesUploaded: 0, AvgPutSize: 0\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"moon\": \"Moon\",\n",
      "  \"capital\": \"The Moon does not have a capital.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "mlflow_client = MlflowClient()\n",
    "config = OpenAIEngineConfig(data={\n",
    "    \"api_key\": \"sk-svcacct-YRsA6cTdakp9lkCfpMh_hd8SmiTetZfn-M_SkJjrQbCQz9RquIajbiBdujKHertqBzZT3BlbkFJp5QdXnRMPYSt4or_JDvYz3IGIpZaNBOz-IlqKT4M7Jt5EQ4fAbCZdqhKH_rMaf6xwAA\",\n",
    "    \"temperature\": 0.5,\n",
    "    \"max_tokens\": 4096,\n",
    "    \"top_p\": 1,\n",
    "    \"frequency_penalty\": 0,\n",
    "    \"presence_penalty\": 0\n",
    "})\n",
    "engine = OpenAIEngine(model_name=\"gpt-4o\", mlflow_client=mlflow_client, config=config)\n",
    "response = await engine.generate_structured(\"What is the capital of the moon?\", structure=\"{moon: str, capital: str}\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:daft_io.stats:IOStatsContext: MicroPartition::concat, Gets: 0, Heads: 0, Lists: 0, BytesRead: 0, AvgGetSize: 0, BytesUploaded: 0, AvgPutSize: 0\n",
      "INFO:daft_io.stats:IOStatsContext: MicroPartition::concat, Gets: 0, Heads: 0, Lists: 0, BytesRead: 0, AvgGetSize: 0, BytesUploaded: 0, AvgPutSize: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-20 04:32:27 arg_utils.py:839] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 09-20 04:32:27 config.py:911] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "INFO 09-20 04:32:27 llm_engine.py:184] Initializing an LLM engine (v0.5.5) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, enable_prefix_caching=False)\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m mlflow_client \u001b[38;5;241m=\u001b[39m MlflowClient()\n\u001b[1;32m      2\u001b[0m config \u001b[38;5;241m=\u001b[39m OpenAIEngineConfig(data\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m4096\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m})\n\u001b[0;32m----> 3\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta-llama/Meta-Llama-3.1-8B-Instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m vllm \u001b[38;5;241m=\u001b[39m VLLM(model\u001b[38;5;241m=\u001b[39mLLM)\n\u001b[1;32m      5\u001b[0m engine \u001b[38;5;241m=\u001b[39m VLLMEngine(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Meta-Llama-3.1-8B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m, mlflow_client\u001b[38;5;241m=\u001b[39mmlflow_client, config\u001b[38;5;241m=\u001b[39mconfig, api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEMPTY\u001b[39m\u001b[38;5;124m\"\u001b[39m, base_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://localhost:8000\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/vllm/entrypoints/llm.py:175\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_context_len_to_capture, max_seq_len_to_capture, disable_custom_all_reduce, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere is no need to pass vision-related arguments anymore.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    154\u001b[0m engine_args \u001b[38;5;241m=\u001b[39m EngineArgs(\n\u001b[1;32m    155\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    156\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    174\u001b[0m )\n\u001b[0;32m--> 175\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[43mLLMEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/vllm/engine/llm_engine.py:473\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    471\u001b[0m executor_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_executor_cls(engine_config)\n\u001b[1;32m    472\u001b[0m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n\u001b[0;32m--> 473\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mengine_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m engine\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/vllm/engine/llm_engine.py:270\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, speculative_config, decoding_config, observability_config, prompt_adapter_config, executor_class, log_stats, usage_context, stat_loggers, input_registry)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_registry \u001b[38;5;241m=\u001b[39m input_registry\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_processor \u001b[38;5;241m=\u001b[39m input_registry\u001b[38;5;241m.\u001b[39mcreate_input_processor(\n\u001b[1;32m    268\u001b[0m     model_config)\n\u001b[0;32m--> 270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_executor \u001b[38;5;241m=\u001b[39m \u001b[43mexecutor_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspeculative_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspeculative_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_adapter_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_adapter_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobservability_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservability_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39membedding_mode:\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_kv_caches()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/vllm/executor/executor_base.py:46\u001b[0m, in \u001b[0;36mExecutorBase.__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, speculative_config, prompt_adapter_config, observability_config)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_adapter_config \u001b[38;5;241m=\u001b[39m prompt_adapter_config\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservability_config \u001b[38;5;241m=\u001b[39m observability_config\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_executor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/vllm/executor/gpu_executor.py:37\u001b[0m, in \u001b[0;36mGPUExecutor._init_executor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initialize the worker and load the model.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_config\u001b[38;5;241m.\u001b[39mworld_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, (\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPUExecutor only supports single GPU.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_worker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_worker\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_worker\u001b[38;5;241m.\u001b[39minit_device()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_worker\u001b[38;5;241m.\u001b[39mload_model()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/vllm/executor/gpu_executor.py:104\u001b[0m, in \u001b[0;36mGPUExecutor._create_worker\u001b[0;34m(self, local_rank, rank, distributed_init_method)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_worker\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    101\u001b[0m                    local_rank: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    102\u001b[0m                    rank: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    103\u001b[0m                    distributed_init_method: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_worker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_create_worker_kwargs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdistributed_init_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdistributed_init_method\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/vllm/executor/gpu_executor.py:23\u001b[0m, in \u001b[0;36mcreate_worker\u001b[0;34m(worker_module_name, worker_class_name, worker_class_fn, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_worker\u001b[39m(worker_module_name: \u001b[38;5;28mstr\u001b[39m, worker_class_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     16\u001b[0m                   worker_class_fn: Optional[Callable[[], Type[WorkerBase]]],\n\u001b[1;32m     17\u001b[0m                   \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     18\u001b[0m     wrapper \u001b[38;5;241m=\u001b[39m WorkerWrapperBase(\n\u001b[1;32m     19\u001b[0m         worker_module_name\u001b[38;5;241m=\u001b[39mworker_module_name,\n\u001b[1;32m     20\u001b[0m         worker_class_name\u001b[38;5;241m=\u001b[39mworker_class_name,\n\u001b[1;32m     21\u001b[0m         worker_class_fn\u001b[38;5;241m=\u001b[39mworker_class_fn,\n\u001b[1;32m     22\u001b[0m     )\n\u001b[0;32m---> 23\u001b[0m     \u001b[43mwrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_worker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\u001b[38;5;241m.\u001b[39mworker\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/vllm/worker/worker_base.py:444\u001b[0m, in \u001b[0;36mWorkerWrapperBase.init_worker\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    441\u001b[0m     mod \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworker_module_name)\n\u001b[1;32m    442\u001b[0m     worker_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(mod, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworker_class_name)\n\u001b[0;32m--> 444\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworker \u001b[38;5;241m=\u001b[39m \u001b[43mworker_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworker \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/vllm/worker/worker.py:99\u001b[0m, in \u001b[0;36mWorker.__init__\u001b[0;34m(self, model_config, parallel_config, scheduler_config, device_config, cache_config, load_config, local_rank, rank, distributed_init_method, lora_config, speculative_config, prompt_adapter_config, is_driver_worker, model_runner_cls, observability_config)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_encoder_decoder_model():\n\u001b[1;32m     98\u001b[0m     ModelRunnerClass \u001b[38;5;241m=\u001b[39m EncoderDecoderModelRunner\n\u001b[0;32m---> 99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_runner: GPUModelRunnerBase \u001b[38;5;241m=\u001b[39m \u001b[43mModelRunnerClass\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkv_cache_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_driver_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_driver_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_adapter_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_adapter_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobservability_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobservability_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mspeculative_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Uninitialized cache engine. Will be initialized by\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# initialize_cache.\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_engine: List[CacheEngine]\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/vllm/worker/model_runner.py:842\u001b[0m, in \u001b[0;36mGPUModelRunnerBase.__init__\u001b[0;34m(self, model_config, parallel_config, scheduler_config, device_config, cache_config, load_config, lora_config, kv_cache_dtype, is_driver_worker, prompt_adapter_config, return_hidden_states, observability_config, input_registry, mm_registry)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph_block_tables \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[1;32m    838\u001b[0m     (\u001b[38;5;28mmax\u001b[39m(_BATCH_SIZES_TO_CAPTURE), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_max_block_per_batch()),\n\u001b[1;32m    839\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32)\n\u001b[1;32m    840\u001b[0m num_attn_heads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mget_num_attention_heads(\n\u001b[1;32m    841\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_config)\n\u001b[0;32m--> 842\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_backend \u001b[38;5;241m=\u001b[39m \u001b[43mget_attn_backend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_attn_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_head_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_kv_heads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_config\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_sliding_window\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkv_cache_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m num_attn_heads \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_backend:\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_backend\u001b[38;5;241m.\u001b[39mget_state_cls()(\n\u001b[1;32m    853\u001b[0m         weakref\u001b[38;5;241m.\u001b[39mproxy(\u001b[38;5;28mself\u001b[39m))\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/vllm/attention/selector.py:108\u001b[0m, in \u001b[0;36mget_attn_backend\u001b[0;34m(num_heads, head_size, num_kv_heads, sliding_window, dtype, kv_cache_dtype, block_size, is_blocksparse)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattention\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mblocksparse_attn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    105\u001b[0m         BlocksparseFlashAttentionBackend)\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m BlocksparseFlashAttentionBackend\n\u001b[0;32m--> 108\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[43mwhich_attn_to_use\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_kv_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m                            \u001b[49m\u001b[43msliding_window\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend \u001b[38;5;241m==\u001b[39m _Backend\u001b[38;5;241m.\u001b[39mFLASH_ATTN:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattention\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflash_attn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m    113\u001b[0m         FlashAttentionBackend)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/vllm/attention/selector.py:215\u001b[0m, in \u001b[0;36mwhich_attn_to_use\u001b[0;34m(num_heads, head_size, num_kv_heads, sliding_window, dtype, kv_cache_dtype, block_size)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# FlashAttn in NVIDIA GPUs.\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m selected_backend \u001b[38;5;241m==\u001b[39m _Backend\u001b[38;5;241m.\u001b[39mFLASH_ATTN:\n\u001b[0;32m--> 215\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcurrent_platform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_device_capability\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m8\u001b[39m:\n\u001b[1;32m    216\u001b[0m         \u001b[38;5;66;03m# Volta and Turing NVIDIA GPUs.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    218\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use FlashAttention-2 backend for Volta and Turing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    219\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPUs.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    220\u001b[0m         selected_backend \u001b[38;5;241m=\u001b[39m _Backend\u001b[38;5;241m.\u001b[39mXFORMERS\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/vllm/platforms/interface.py:28\u001b[0m, in \u001b[0;36mPlatform.get_device_capability\u001b[0;34m(device_id)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_capability\u001b[39m(device_id: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mlflow_client = MlflowClient()\n",
    "config = OpenAIEngineConfig(data={\"temperature\": 0.5, \"max_tokens\": 4096, \"top_p\": 1, \"frequency_penalty\": 0, \"presence_penalty\": 0})\n",
    "llm = LLM(model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "vllm = VLLM(model=LLM)\n",
    "engine = VLLMEngine(model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", mlflow_client=mlflow_client, config=config, api_key=\"EMPTY\", base_url=\"http://localhost:8000\")\n",
    "\n",
    "paligemma = vllm(\n",
    "    model_name=\"google/paligemma-3b-mix-448\",\n",
    "    engine=engine,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from beta.prompts import prompt\n",
    "\n",
    "@prompt\n",
    "def my_prompt(question):\n",
    "    \"\"\"{{ question }}\"\"\"\n",
    "\n",
    "@prompt\n",
    "def cot(my_prompt, max_steps):\n",
    "    \"\"\"\n",
    "    System: \n",
    "        Using Chain of Thought:\n",
    "        max_steps = {{max_steps}}\n",
    "\n",
    "        for step in max_steps:\n",
    "            Reason about the request and append, \"Therefore\"\n",
    "        \n",
    "        Make a concluding response \n",
    "\n",
    "    User:\n",
    "    {{ my_prompt }}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "from datetime import date\n",
    "from ray import serve\n",
    "@serve.deployment(name=\"Test0\")\n",
    "class QuarterlyEarnings(BaseModel):\n",
    "    company_name: str = Field(..., description=\"Name of the company\")\n",
    "    fiscal_quarter: int = Field(..., ge=1, le=4, description=\"Fiscal quarter (1-4)\")\n",
    "    fiscal_year: int = Field(..., description=\"Fiscal year\")\n",
    "    revenue: float = Field(..., description=\"Total revenue for the quarter\")\n",
    "    net_income: float = Field(..., description=\"Net income for the quarter\")\n",
    "    earnings_per_share: float = Field(..., description=\"Earnings per share\")\n",
    "    report_date: date = Field(..., description=\"Date of the earnings report\")\n",
    "    analyst_expectations_met: Optional[bool] = Field(None, description=\"Whether analyst expectations were met\")\n",
    "    key_highlights: Optional[List[str]] = Field(None, description=\"Key highlights from the earnings report\")\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return \"QuarterlyEarnings deployment is working\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-20 05:06:47,641\tINFO worker.py:1619 -- Calling ray.init() again after it has already been called.\n",
      "INFO 2024-09-20 05:06:47,645 serve 1767713 api.py:259 - Connecting to existing Serve app in namespace \"serve\". New http options will not be applied.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Deploying application Test0 failed: The deployments ['Test0'] are UNHEALTHY.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m ray\u001b[38;5;241m.\u001b[39minit(ignore_reinit_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, logging_level\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDEBUG\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m app \u001b[38;5;241m=\u001b[39m QuarterlyEarnings\u001b[38;5;241m.\u001b[39mbind()\n\u001b[0;32m----> 8\u001b[0m handle \u001b[38;5;241m=\u001b[39m \u001b[43mserve\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTest0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m ray\u001b[38;5;241m.\u001b[39mget(handle\u001b[38;5;241m.\u001b[39mremote())\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Add debug print statements\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/ray/serve/api.py:543\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(target, blocking, name, route_prefix, logging_config)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;129m@PublicAPI\u001b[39m(stability\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\n\u001b[1;32m    511\u001b[0m     target: Application,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    515\u001b[0m     logging_config: Optional[Union[Dict, LoggingConfig]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    516\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DeploymentHandle:\n\u001b[1;32m    517\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run an application and return a handle to its ingress deployment.\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \n\u001b[1;32m    519\u001b[0m \u001b[38;5;124;03m    The application is returned by `Deployment.bind()`. Example:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;124;03m        DeploymentHandle: A handle that can be used to call the application.\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m     handle \u001b[38;5;241m=\u001b[39m \u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroute_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mroute_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogging_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogging_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    549\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeployed app \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m blocking:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/ray/serve/api.py:494\u001b[0m, in \u001b[0;36m_run\u001b[0;34m(target, _blocking, name, route_prefix, logging_config)\u001b[0m\n\u001b[1;32m    483\u001b[0m     deployment_parameters \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    484\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: deployment\u001b[38;5;241m.\u001b[39m_name,\n\u001b[1;32m    485\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplica_config\u001b[39m\u001b[38;5;124m\"\u001b[39m: deployment\u001b[38;5;241m.\u001b[39m_replica_config,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mingress\u001b[39m\u001b[38;5;124m\"\u001b[39m: deployment\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m==\u001b[39m ingress\u001b[38;5;241m.\u001b[39m_name,\n\u001b[1;32m    492\u001b[0m     }\n\u001b[1;32m    493\u001b[0m     parameter_group\u001b[38;5;241m.\u001b[39mappend(deployment_parameters)\n\u001b[0;32m--> 494\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeploy_application\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameter_group\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ingress \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;66;03m# The deployment state is not guaranteed to be created after\u001b[39;00m\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;66;03m# deploy_application returns; the application state manager will\u001b[39;00m\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;66;03m# need another reconcile iteration to create it.\u001b[39;00m\n\u001b[1;32m    504\u001b[0m     client\u001b[38;5;241m.\u001b[39m_wait_for_deployment_created(ingress\u001b[38;5;241m.\u001b[39mname, name)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/ray/serve/_private/client.py:46\u001b[0m, in \u001b[0;36m_ensure_connected.<locals>.check\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown:\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RayServeException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClient has already been shut down.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/ray/serve/_private/client.py:281\u001b[0m, in \u001b[0;36mServeControllerClient.deploy_application\u001b[0;34m(self, name, deployments, _blocking)\u001b[0m\n\u001b[1;32m    279\u001b[0m ray\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_controller\u001b[38;5;241m.\u001b[39mdeploy_application\u001b[38;5;241m.\u001b[39mremote(name, deployment_args_list))\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _blocking:\n\u001b[0;32m--> 281\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_application_running\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m deployment \u001b[38;5;129;01min\u001b[39;00m deployments:\n\u001b[1;32m    283\u001b[0m         deployment_name \u001b[38;5;241m=\u001b[39m deployment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/ray/serve/_private/client.py:229\u001b[0m, in \u001b[0;36mServeControllerClient._wait_for_application_running\u001b[0;34m(self, name, timeout_s)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m status\u001b[38;5;241m.\u001b[39mapp_status\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m ApplicationStatus\u001b[38;5;241m.\u001b[39mDEPLOY_FAILED:\n\u001b[0;32m--> 229\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    230\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeploying application \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstatus\u001b[38;5;241m.\u001b[39mapp_status\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    231\u001b[0m     )\n\u001b[1;32m    233\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWaiting for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to be RUNNING, current status: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstatus\u001b[38;5;241m.\u001b[39mapp_status\u001b[38;5;241m.\u001b[39mstatus\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    236\u001b[0m )\n\u001b[1;32m    237\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(CLIENT_POLLING_INTERVAL_S)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Deploying application Test0 failed: The deployments ['Test0'] are UNHEALTHY."
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray import serve\n",
    "import asyncio\n",
    "\n",
    "ray.init(ignore_reinit_error=True, logging_level=\"DEBUG\")\n",
    "app = QuarterlyEarnings.bind()\n",
    "\n",
    "handle = serve.run(app, name=\"Test0\")\n",
    "\n",
    "ray.get(handle.remote())\n",
    "\n",
    "# Add debug print statements\n",
    "print(\"Ray initialized and Serve started\")\n",
    "\n",
    "from ray.serve.handle import DeploymentHandle\n",
    "import ray.serve as serve\n",
    "from beta.agents.agent import Metadata\n",
    "from beta.inference.inference_engine import RayInferenceEngine\n",
    "\n",
    "\n",
    "\n",
    "q1_agent = Agent(\n",
    "    name=\"Q1 Agent\",\n",
    "    model=handle,\n",
    "    embedding=handle,\n",
    "    stt=handle,\n",
    "    tools=[CalculatorTool, DataFrameTool],\n",
    "    metadata=Metadata(name=\"Q1 Agent\", description=\"Agent for Q1 Agent\"),\n",
    "    inference_adapter=RayInferenceEngine(handle),\n",
    "    is_async=True\n",
    ")\n",
    "\n",
    "async def run_agent():\n",
    "    try:\n",
    "        print(\"Preparing to call agent\")\n",
    "        response = await q1_agent(\n",
    "            prompt=my_prompt(\"What were the key highlights of the Q4 2024 earnings?\"),\n",
    "        )\n",
    "        print(\"Agent response received:\")\n",
    "        print(response)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while calling agent: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Run the async function\n",
    "asyncio.get_event_loop().run_until_complete(run_agent())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available deployments:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'ray.serve' has no attribute 'list_deployments'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvailable deployments:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mserve\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_deployments\u001b[49m())\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m serve\u001b[38;5;241m.\u001b[39mlist_deployments():\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest0 deployment not found. Creating a dummy deployment for testing.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'ray.serve' has no attribute 'list_deployments'"
     ]
    }
   ],
   "source": [
    "print(\"Available deployments:\")\n",
    "print(serve.list_deployments())\n",
    "\n",
    "if \"Test0\" not in serve.list_deployments():\n",
    "    print(\"Test0 deployment not found. Creating a dummy deployment for testing.\")\n",
    "    \n",
    "    @serve.deployment(name=\"Test0\")\n",
    "    class DummyModel:\n",
    "        async def generate(self, prompt=None, messages=None):\n",
    "            return \"This is a dummy response for testing purposes.\"\n",
    "    \n",
    "    DummyModel.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (model_object.py, line 15)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3577\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[26], line 1\u001b[0m\n    from beta.models.obj import llm\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m~/beta/beta/models/obj/__init__.py:1\u001b[0;36m\n\u001b[0;31m    from .model_object import BaseMLModel, ModelObject\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m~/beta/beta/models/obj/model_object.py:15\u001b[0;36m\u001b[0m\n\u001b[0;31m    from beta.data.obj.base. import DataObjectManager\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from beta.models.obj import llm\n",
    "from beta.models.serve.engines import LitServeEngine, OpenAIEngine, OpenRouterEngine, VLLMEngine\n",
    "\n",
    "\n",
    "completion_settings = llm.settings(\n",
    "    temperature=0.5,\n",
    "    max_tokens=4096,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    ")\n",
    "\n",
    "# Defaults to OpenRouter\n",
    "# Expects OPENROUTER_API_KEY to be defined in .env\n",
    "gemini_free = llm(\n",
    "    model_name=\"google/gemini-pro-1.5-flash\",\n",
    "    settings=completion_settings,\n",
    ")\n",
    "\n",
    "# If OPENAI_API_KEY, uses { OpenAi | AsyncOpenAI } clients instead of OpenRouter. \n",
    "gpt4o = llm(\n",
    "    model_name=\"openai/o1-mini\",\n",
    "    settings=completion_settings,\n",
    ")\n",
    "\n",
    "# If ANTHROPIC_API_KEY, uses {Anthropic | AsyncAnthopic }\n",
    "sonnet = llm(\n",
    "    model_name=\"anthopic/claude-3.5-sonnet\",\n",
    "    settings = completion_settings,\n",
    ")\n",
    "\n",
    "\n",
    "# Specifying the Engine with OS Model\n",
    "paligemma = llm(\n",
    "    model_name=\"google/paligemma-3b-mix-448\",\n",
    "    engine=LitServeEngine(), \n",
    "    completion_settings=completion_settings\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from beta.tools import CalculatorTool, DataframeTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from beta.agents import BaseAgent, SpeechAgent\n",
    "\n",
    "artifact_df = daft.DataFrame()\n",
    "node_df = daft.DataFrame()\n",
    "document_df = daft.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "earnings = Data.Projects.get(\"Earnings Q4 2024\")\n",
    "knowledge_graph = my_project.Graphs.get(\"latest\")\n",
    "\n",
    "\n",
    "q1_agent = Agent(\n",
    "    llm=paligemma,\n",
    "    tools=[CalculatorTool, DataframeTool],\n",
    "    context=[earnings],\n",
    "    memory={'short':[knowl]},\n",
    "    planning=\n",
    "    rethinking=\n",
    ")\n",
    "\n",
    "\n",
    "@prompt\n",
    "def my_prompt(question):\n",
    "    \"\"\"{{ question}}\"\"\"\n",
    "\n",
    "@prompt\n",
    "def cot(my_prompt,max_steps):\n",
    "    \"\"\"\n",
    "    System: \n",
    "        Using Chain of Thought:\n",
    "        max_steps = {{max_steps}}\n",
    "\n",
    "        for step in max_steps:\n",
    "            Reason about the request and append, \"Therefore\"\n",
    "        \n",
    "        Make a concluding response \n",
    "\n",
    "    User:\n",
    "    {{ my_prompt }}\n",
    "    \"\"\"\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "from datetime import date\n",
    "\n",
    "class QuarterlyEarnings(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents quarterly earnings data for a company.\n",
    "    \"\"\"\n",
    "    company_name: str = Field(..., description=\"Name of the company\")\n",
    "    fiscal_quarter: int = Field(..., ge=1, le=4, description=\"Fiscal quarter (1-4)\")\n",
    "    fiscal_year: int = Field(..., description=\"Fiscal year\")\n",
    "    revenue: float = Field(..., description=\"Total revenue for the quarter\")\n",
    "    net_income: float = Field(..., description=\"Net income for the quarter\")\n",
    "    earnings_per_share: float = Field(..., description=\"Earnings per share\")\n",
    "    report_date: date = Field(..., description=\"Date of the earnings report\")\n",
    "    analyst_expectations_met: Optional[bool] = Field(None, description=\"Whether analyst expectations were met\")\n",
    "    key_highlights: Optional[List[str]] = Field(None, description=\"Key highlights from the earnings report\")\n",
    "\n",
    "\n",
    "response = await my_agent(\n",
    "    prompt=\n",
    "    response_model=[Address], \n",
    "        reponse_model_strict = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swarm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
